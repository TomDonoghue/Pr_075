{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Background\n",
    "\n",
    "\n",
    "### Research Question\n",
    "What are the factors that negatively and positively affect crime rates in San Diego County the most? Factors we are going to research include street lamps lighting coverage, processed Alcohol permit records distribution, pollution burden score, education rate, poverty rate, unemployment rate, single mother rate, traffic injury numbers. Then how to most effectively prevent crime based on the analysis to this question?\n",
    "\n",
    "\n",
    "### Hypothesis\n",
    "We predict that higher street lamps lighting coverage would cause lower crime rates; higher processed Alcohol permit records would cause higher crime rates; higher pollution burden score would cause higher crime rates; higher education rate would cause lower crime rates; higher poverty rate would cause higher crime rates; higher unemployment rate would cause higher crime rates; higher single mother rate would cause higher crime rates; higher traffic injury numbers would cause higher crime rates. And the most effective way to prevent crime would be to find the factor that relates to crime rates the most closely and to change that factor in a good way to reduce crime rate.\n",
    "\n",
    "\n",
    "### Background & Prior Work\n",
    "- We first started our project proposal question as “would the occurrence of street light prevent crimes?” since one of our friends once got robbed on a street without street lamps, and that’s the reason why we are interested in the topic. \n",
    "- If there were street lamps on the street our friend got robbed, he may be able to identify the physical appearance of the criminal, so that it is easier for the police to resolve the case. Besides, the existence of street light might make the potential criminals aware of higher risk of being arrested and thus prevent the crime.\n",
    "- When we start to find data sets and dig into more depth of the question, we realized that there are many other factors that would also affect crime rates at the same time and it is impossible to ignore the other factors. So we finally decide to also analyze the other factors that we can find in data sets and see which of these correlate with crime rates the most.\n",
    "- The question we have is important because it is related to the safety issue of people’s living and people pay lots of attention and emphasis on safety and try to lower the crime rate. Finding out which factor is related to crime rate the most would suggests an effective way to reduce crime rate and thus improve people’s living conditions.\n",
    "\n",
    "\n",
    "### Proposed Methods\n",
    "#### Data Cleaning\n",
    "- The data sets we found so far are not clean at all. Since we have four different data sets for now, we have to do data cleaning for each of them and there are many columns of irrelevant information in each data set which we do not have to use for our project, especially the “San Diego Census Tracts” data set. For example, dates, types of crimes. These are the columns that we want to clean before running our data analysis because our data sets are really big and running our code on irrelevant data would slow down our entire process. However, here is a conflict which is that the factors which we do not think relate to crime rates for now might become related after our data analysis. So we want to keep as many data records as possible so that we could always get access to these data.\n",
    "\n",
    "- We plan to use Pandas to process our data sets and do data cleaning such as dropping columns and replace outliers data. For example of data cleaning in “San Diego Street Lamp Locations” data set, we will remove all irrelevant columns such as brand of street lamp, clean columns by removing day crimes, and crimes  that are irrelevant such as DUIs, fraud, etc. We need to to further throw out outliers in the dataset such as recording errors. Then we might need to do random sampling for at least one of our data sets because that data set has 662403 rows which is really slow even for our laptop to open it. We do not want this huge data set to slow down our data analysis too much.\n",
    "\n",
    "#### Data Analysis\n",
    "- We will need to use Google Map APIs to import our data and present them on San Diego County map. For example for the street lamp data set, if the street lamp data includes highway data, we might need to manually scrape out highway data based off of a highway dataset. Besides, we will analyze if the distribution of streetlamp is related to distribution of crimes. Basically, we will have a coverage of streetlamp calculated as a circle area by setting up a radius and combining it with the longitude and latitude of the streetlamp. Then we will traverse through all the cleaned data records of crimes to compare if the location of where the crime happened is within the area of streetlamp light coverage or not. Finally, we will get how many percentage of crimes were actually happened under the coverage of street lights and how many were happened in the dark. We will do this set of analysis for other data too. For example across different neighbourhood/communities with different poverty rate and other rates, how many crime acts occurred. In order to get an intuitive sense of the result, we will use Google Map API and some other Python geographic analysis packages to draw the San Diego County map out, and then plot the neighbourhood/communities with different colors to represent number of crimes, and also create other heat maps to plot these communities with different colors to represent other data. Finally lay them out together to see what factors correlates with number of crimes the most. We think the heat map style (Similiar to the one shown in the Monday guest lecture regarding Pop star's’ popularity across different regions) is the most convenient and intuitive way of seeing the relationship.\n",
    "\n",
    "- We will also work on plotting data out to find correlations. We will plot the amount of crime out along with other data like poverty rate, unemployment rate across different neighbourhood/communities and then check if the graph looks alike or not.\n",
    "- Since we will be mainly using longitude and latitude for analysis, we do not need any data transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Description\n",
    "\n",
    "### Datasets\n",
    "Dataset 1:\n",
    "- Dataset Name: San Diego Street Lamp Locations\n",
    "- Link to the dataset: https://s3.amazonaws.com/s3.sandiegodata.org/repo/clarinova.com/street_lights-casnd-429e-r1/street_lights.csv\n",
    "- Number of observations: 48454 \n",
    "- 1-2 sentences describing the dataset: A data set including latitude and longitude of street lamps, community, and status of lamp.\n",
    "\n",
    "\n",
    "Dataset 2:\n",
    "- Dataset Name: San Diego Region Crime Incidents 2007 - 2013\n",
    "- Link to the dataset: https://s3.amazonaws.com/s3.sandiegodata.org/repo/clarinova.com/crime-incidents-casnd-7ba4-r3/incidents-5y.csv\n",
    "- Number of observations: 662403\n",
    "- 1-2 sentences describing the dataset: A very comprehensive data set including crimes in San Diego. Data includes date of crime, time of crime, type of crime, location of crime (address, neighbourhood, latitude and longitude), and other various types of data.\n",
    "\n",
    "Dataset 3:\n",
    "- Dataset Name: San Diego County Alcohol Permits\n",
    "- Link to the dataset: https://s3.amazonaws.com/s3.sandiegodata.org/repo/clarinova.com/alcohol_licenses-casnd-429e-r1/abs-licenses-casnd.csv\n",
    "- Number of observations: 4987\n",
    "- 1-2 sentences describing the dataset: A data set which records information of distribution of processed San Diego County Alcohol permits. Data includes issue date, expiration date, owner, address, longitude, latitude, business type.\n",
    "\n",
    "Dataset 4:\n",
    "- Dataset Name: San Diego Census Tracts\n",
    "- Link to the dataset: http://ds.civicknowledge.org.s3.amazonaws.com/sandiegodata.org/sandiegocensustract.xlsx\n",
    "- Number of observations: 625\n",
    "- 1-2 sentences describing the dataset: A data set which records information of various San Diego Census Tracts. Data includes neighbourhood names, zip code, total population, single mother rate, poverty rate, pollution burden score, education rate, unemployment rate,  longitude, latitude, traffic injury numbers.\n",
    "\n",
    "\n",
    "Plan to combine these datasets: We plan on cleaning the datasets first by filtering out unnecessary info such as street lamp brand, crimes committed during the day, unrelevant crimes such as DUI, etc.. The key point to connect all of our data sets is that all the data sets we are going to use have longitude and latitude. So we will link the latitude and longitude in different data sets together so that we will be able to geographically connect all of the datas we need and pinpoint the location of each row of observation and each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning/Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary libraries, then import csv and clean all unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "import re as re\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in randomly sampled crime data and drop all unrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This df_crime is complete with only na values dropped.\n",
    "df_crime = pd.read_csv('./Data/incidents-100k.csv')\n",
    "df_crime = df_crime[['date','is_night','type','lat','lon']]\n",
    "df_crime = df_crime.dropna()\n",
    "# print (df_crime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all crime types and remove all crimes that are not physical. We did this because we are relating street crimes locations with other factors such as street lamp location and unemployment rates which are location sensitive, thus all non-physical crimes are not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRUGS/ALCOHOL VIOLATIONS    23198\n",
      "THEFT/LARCENY               13211\n",
      "VEHICLE BREAK-IN/THEFT      12586\n",
      "MOTOR VEHICLE THEFT         10621\n",
      "BURGLARY                     8787\n",
      "ASSAULT                      8012\n",
      "VANDALISM                    7624\n",
      "FRAUD                        5412\n",
      "DUI                          4917\n",
      "ROBBERY                      2243\n",
      "SEX CRIMES                   2122\n",
      "WEAPONS                       997\n",
      "ARSON                         203\n",
      "HOMICIDE                       67\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "crime_type = df_crime['type'].value_counts()\n",
    "print (crime_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove FRAUD crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date  is_night                      type        lat         lon\n",
      "0      2008-09-19         0                   WEAPONS  32.957337 -117.143777\n",
      "2      2009-05-25         1                   ASSAULT  32.796761 -117.254577\n",
      "3      2011-04-22         0  DRUGS/ALCOHOL VIOLATIONS  32.757287 -117.129870\n",
      "4      2012-05-12         1                   ASSAULT  32.836098 -117.206645\n",
      "5      2010-12-21         0                   ROBBERY  32.820347 -117.182419\n",
      "6      2009-09-23         1                SEX CRIMES  32.707287 -117.158730\n",
      "7      2008-10-12         1                  BURGLARY  32.836803 -117.208681\n",
      "8      2008-06-06         0    VEHICLE BREAK-IN/THEFT  32.774932 -117.144025\n",
      "9      2007-03-20         0  DRUGS/ALCOHOL VIOLATIONS  32.755078 -117.099457\n",
      "10     2011-05-13         0    VEHICLE BREAK-IN/THEFT  32.791932 -117.126213\n",
      "11     2011-09-12         1                     ARSON  32.749915 -117.162472\n",
      "12     2007-03-16         1    VEHICLE BREAK-IN/THEFT  32.819463 -117.110901\n",
      "13     2007-10-10         0                   ASSAULT  32.911808 -117.102657\n",
      "14     2008-12-23         0  DRUGS/ALCOHOL VIOLATIONS  32.708885 -117.153727\n",
      "15     2007-04-03         1  DRUGS/ALCOHOL VIOLATIONS  32.716243 -117.155610\n",
      "16     2007-04-25         1       MOTOR VEHICLE THEFT  32.744593 -117.225853\n",
      "17     2008-01-28         0                  BURGLARY  32.904084 -117.133818\n",
      "18     2010-12-29         0                 VANDALISM  32.919065 -117.084075\n",
      "19     2009-12-15         0       MOTOR VEHICLE THEFT  32.751673 -117.161503\n",
      "20     2008-05-14         0                 VANDALISM  32.714144 -117.159247\n",
      "21     2007-11-13         1    VEHICLE BREAK-IN/THEFT  32.576867 -117.024998\n",
      "22     2012-07-19         0  DRUGS/ALCOHOL VIOLATIONS  32.707843 -117.148545\n",
      "23     2010-02-03         1                SEX CRIMES  32.787687 -117.097248\n",
      "24     2007-10-04         1    VEHICLE BREAK-IN/THEFT  32.795650 -117.237591\n",
      "25     2008-05-16         1  DRUGS/ALCOHOL VIOLATIONS  32.772029 -117.252752\n",
      "26     2009-02-16         1                       DUI  32.727536 -117.126406\n",
      "27     2007-10-22         0             THEFT/LARCENY  32.738983 -117.173029\n",
      "28     2010-11-09         1  DRUGS/ALCOHOL VIOLATIONS  32.553284 -117.042711\n",
      "29     2007-04-10         1  DRUGS/ALCOHOL VIOLATIONS  32.753180 -117.101651\n",
      "30     2007-02-23         1  DRUGS/ALCOHOL VIOLATIONS  32.757112 -117.084414\n",
      "...           ...       ...                       ...        ...         ...\n",
      "99970  2008-09-22         0  DRUGS/ALCOHOL VIOLATIONS  32.786403 -117.121536\n",
      "99971  2007-04-27         1                  BURGLARY  32.724375 -117.078793\n",
      "99972  2008-11-15         0                SEX CRIMES  32.901549 -117.112931\n",
      "99973  2007-06-08         1       MOTOR VEHICLE THEFT  32.717113 -117.140284\n",
      "99974  2007-07-04         1    VEHICLE BREAK-IN/THEFT  33.037848 -117.063215\n",
      "99975  2011-04-22         0                   ROBBERY  32.686201 -117.049475\n",
      "99976  2010-07-20         1                  BURGLARY  32.763805 -117.135146\n",
      "99977  2011-09-28         1                   ASSAULT  32.707843 -117.148545\n",
      "99978  2008-01-08         1                       DUI  32.710428 -117.157835\n",
      "99979  2012-05-06         0    VEHICLE BREAK-IN/THEFT  32.906816 -117.175656\n",
      "99980  2007-04-15         1                       DUI  32.769744 -117.195219\n",
      "99981  2007-06-21         0                 VANDALISM  32.966468 -117.098567\n",
      "99982  2010-10-16         0             THEFT/LARCENY  32.748458 -117.133741\n",
      "99983  2011-06-05         0    VEHICLE BREAK-IN/THEFT  32.750490 -117.096569\n",
      "99984  2012-12-07         0       MOTOR VEHICLE THEFT  32.730591 -117.100959\n",
      "99985  2007-11-28         1  DRUGS/ALCOHOL VIOLATIONS  32.708511 -117.126322\n",
      "99986  2009-05-09         0             THEFT/LARCENY  32.883068 -117.154352\n",
      "99987  2010-02-07         0  DRUGS/ALCOHOL VIOLATIONS  32.565826 -117.063186\n",
      "99988  2011-05-24         1                  BURGLARY  32.879570 -117.160608\n",
      "99989  2009-04-18         0  DRUGS/ALCOHOL VIOLATIONS  32.718272 -117.221185\n",
      "99990  2009-08-15         0    VEHICLE BREAK-IN/THEFT  32.762999 -117.127912\n",
      "99991  2011-06-10         0             THEFT/LARCENY  32.719377 -117.160189\n",
      "99992  2008-01-17         1    VEHICLE BREAK-IN/THEFT  32.744389 -117.220570\n",
      "99993  2007-03-14         1       MOTOR VEHICLE THEFT  32.545436 -117.043212\n",
      "99994  2009-11-14         1  DRUGS/ALCOHOL VIOLATIONS  32.711495 -117.159666\n",
      "99995  2009-11-14         1    VEHICLE BREAK-IN/THEFT  32.731875 -117.165752\n",
      "99996  2012-03-11         1       MOTOR VEHICLE THEFT  32.586207 -117.016643\n",
      "99997  2010-11-12         1  DRUGS/ALCOHOL VIOLATIONS  32.731123 -117.097050\n",
      "99998  2008-08-12         0             THEFT/LARCENY  32.814715 -117.062826\n",
      "99999  2008-06-08         1                 VANDALISM  32.802782 -117.218064\n",
      "\n",
      "[94588 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#This df_crime has fraud data and na data removed\n",
    "df_crime = df_crime[df_crime.type != \"FRAUD\"]\n",
    "print (df_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we import street_light data. We do not need to remove any additional rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lat         lon\n",
      "0      32.766618 -117.236854\n",
      "1      32.766391 -117.234878\n",
      "2      32.727515 -117.158183\n",
      "3      32.725645 -117.154312\n",
      "4      32.719670 -117.108944\n",
      "5      32.719685 -117.108724\n",
      "6      32.719280 -117.109084\n",
      "7      32.719316 -117.108864\n",
      "8      32.757065 -117.146967\n",
      "9      32.756942 -117.146445\n",
      "10     32.756748 -117.146439\n",
      "11     32.756396 -117.146444\n",
      "12     32.796264 -117.082365\n",
      "13     32.796625 -117.081877\n",
      "14     32.796722 -117.081907\n",
      "15     32.795939 -117.082294\n",
      "16     32.795658 -117.081646\n",
      "17     32.794265 -117.080834\n",
      "18     32.794242 -117.080516\n",
      "19     32.793862 -117.081514\n",
      "20     32.763081 -117.117993\n",
      "21     32.763031 -117.117450\n",
      "22     32.805300 -117.147254\n",
      "23     32.805712 -117.147033\n",
      "24     32.805524 -117.147407\n",
      "25     32.679223 -117.036293\n",
      "26     32.679638 -117.035682\n",
      "27     32.680097 -117.034970\n",
      "28     32.680408 -117.035497\n",
      "29     32.680169 -117.035546\n",
      "...          ...         ...\n",
      "48425  32.752595 -117.250439\n",
      "48426  32.727885 -117.158092\n",
      "48427  32.728718 -117.157581\n",
      "48428  32.717025 -117.123683\n",
      "48429  32.779072 -117.211898\n",
      "48430  32.765017 -117.131546\n",
      "48431  32.783930 -117.208908\n",
      "48432  32.751058 -117.101871\n",
      "48433  32.710492 -117.165403\n",
      "48434  32.757963 -117.127603\n",
      "48435  32.778743 -117.249756\n",
      "48436  32.742890 -117.119413\n",
      "48437  32.874183 -117.205410\n",
      "48438  32.874366 -117.205438\n",
      "48439  32.901732 -117.122810\n",
      "48440  32.879023 -117.176237\n",
      "48441  32.879052 -117.174897\n",
      "48442  32.808128 -117.124743\n",
      "48443  32.808003 -117.124455\n",
      "48444  33.016206 -117.148265\n",
      "48445  32.732978 -117.228377\n",
      "48446  32.734833 -117.093489\n",
      "48447  32.759096 -117.117972\n",
      "48448  32.721571 -117.165501\n",
      "48449  32.706427 -117.066665\n",
      "48450  32.729824 -117.172334\n",
      "48451  32.897775 -117.143868\n",
      "48452  32.994661 -117.117880\n",
      "48453  32.995370 -117.117264\n",
      "48454  32.792494 -117.054855\n",
      "\n",
      "[48455 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# This df_lights has only lat and lon data with na dropped.\n",
    "df_lights = pd.read_csv('./Data/street_lights.csv')\n",
    "df_lights = df_lights[['lat','lon']]\n",
    "df_lights = df_lights.dropna()\n",
    "print (df_lights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we import census tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TRACTNUM  SingMother  PovertyRt  TotalPopulation   Longitude  \\\n",
      "1    6073020028    0.221421      0.370           3479.0 -117.170098   \n",
      "2    6073004501    0.048000      0.151           2875.0 -117.138174   \n",
      "3    6073004800    0.198259      0.428           4115.0 -117.138140   \n",
      "4    6073003601    0.317631      0.469           3250.0 -117.116515   \n",
      "5    6073018603    0.150833      0.186           6810.0 -117.358627   \n",
      "6    6073020207    0.159590      0.190           4765.0 -117.053978   \n",
      "7    6073019501    0.166667      0.188           4147.0 -117.249598   \n",
      "8    6073019502    0.139986      0.260           5715.0 -117.240665   \n",
      "9    6073003602    0.224571      0.377           3079.0 -117.107411   \n",
      "10   6073018000    0.011771      0.079           3711.0 -117.349985   \n",
      "11   6073020029    0.058417      0.175           4819.0 -117.189465   \n",
      "13   6073007702    0.012237      0.125           3849.0 -117.235014   \n",
      "14   6073000400    0.010101      0.166           3669.0 -117.163058   \n",
      "15   6073005100    0.057279      0.480           7140.0 -117.152808   \n",
      "16   6073019405    0.094324      0.197           3592.0 -117.275994   \n",
      "17   6073004700    0.212894      0.452           1858.0 -117.145075   \n",
      "18   6073020214    0.143658      0.476           5143.0 -117.084424   \n",
      "19   6073020601    0.251493      0.342           5560.0 -117.077951   \n",
      "20   6073019206    0.180074      0.124           5614.0 -117.237259   \n",
      "21   6073007800    0.051571      0.126           5724.0 -117.224340   \n",
      "22   6073009000    0.100987      0.310           3861.0 -117.178828   \n",
      "23   6073003502    0.229388      0.403           4946.0 -117.113490   \n",
      "24   6073004100    0.048276      0.260           6546.0 -117.129249   \n",
      "25   6073020109    0.044164      0.223           5127.0 -117.069255   \n",
      "26   6073005700    0.000000      0.408           1948.0 -117.162561   \n",
      "27   6073008600    0.157981      0.244           6864.0 -117.177940   \n",
      "28   6073005900    0.012433      0.123           2821.0 -117.164267   \n",
      "29   6073018200    0.095278      0.269           6847.0 -117.370238   \n",
      "30   6073020211    0.091659      0.174           6702.0 -117.064017   \n",
      "31   6073004600    0.005666      0.188           1940.0 -117.144902   \n",
      "..          ...         ...        ...              ...         ...   \n",
      "590  6073009106    0.015608      0.092           3882.0 -117.198252   \n",
      "591  6073020810    0.004592      0.013           5266.0 -116.787580   \n",
      "592  6073008310    0.044059      0.057           5392.0 -117.245816   \n",
      "593  6073017054    0.058342      0.042           5810.0 -117.045904   \n",
      "594  6073008324    0.010737      0.025           6600.0 -117.250278   \n",
      "595  6073008333    0.065700      0.041          13748.0 -117.209222   \n",
      "596  6073017022    0.023710      0.060           5316.0 -117.104372   \n",
      "597  6073017032    0.038347      0.016          13593.0 -117.104427   \n",
      "598  6073017106    0.057464      0.085           4973.0 -117.211881   \n",
      "599  6073017601    0.036798      0.063           4941.0 -117.278705   \n",
      "600  6073017303    0.033476      0.105           3018.0 -117.263628   \n",
      "601  6073020016    0.030534      0.140           9314.0 -117.252776   \n",
      "602  6073017014    0.000000      0.072           2416.0 -117.068221   \n",
      "603  6073008362    0.018361      0.209           3141.0 -117.233409   \n",
      "604  6073021500    0.084564      0.025           8846.0 -117.188965   \n",
      "605  6073019804    0.034706      0.110           4447.0 -117.311734   \n",
      "606  6073002801    0.009535      0.556           3068.0 -117.083790   \n",
      "607  6073015200    0.048319      0.013           3879.0 -116.989361   \n",
      "608  6073000100    0.012087      0.034           3029.0 -117.185894   \n",
      "609  6073008311    0.033399      0.007           2884.0 -117.263326   \n",
      "610  6073018700    0.121340      0.107          37452.0 -117.420557   \n",
      "611  6073008200    0.041123      0.086           3022.0 -117.275750   \n",
      "612  6073008327    0.055654      0.050           5775.0 -117.242097   \n",
      "613  6073009103    0.057766      0.051           3679.0 -117.195642   \n",
      "614  6073017051    0.041109      0.037           4301.0 -117.066506   \n",
      "615  6073002001    0.053430      0.039           3354.0 -117.104299   \n",
      "617  6073009510    0.074367      0.088           5087.0 -117.109307   \n",
      "618  6073009801    0.041551      0.075           4894.0 -117.030735   \n",
      "620  6073017053    0.041386      0.033           3364.0 -117.031538   \n",
      "621  6073021600    0.043850      0.069           3391.0 -117.156137   \n",
      "\n",
      "      Latitude  PollutionBurdenScore  Education  Unemployment  \n",
      "1    33.141805              3.410794       55.1          4.31  \n",
      "2    32.717632              4.277644       18.0          5.66  \n",
      "3    32.708456              4.593304       47.0         10.62  \n",
      "4    32.690986              6.646059       65.3         17.84  \n",
      "5    33.211511              4.675128       35.6          8.64  \n",
      "6    33.133471              3.218052       47.8         12.91  \n",
      "7    33.208474              3.045434       55.6         11.52  \n",
      "8    33.207787              3.147138       60.5          8.69  \n",
      "9    32.692937              4.870778       58.7         12.67  \n",
      "10   33.154791              3.148714        1.4          7.33  \n",
      "11   33.139222              6.178146       35.0          5.68  \n",
      "13   32.789837              2.514971        1.5         11.17  \n",
      "14   32.753332              4.753679        3.0          6.62  \n",
      "15   32.703738              6.501927       21.6         25.23  \n",
      "16   33.196565              3.398308       32.7         10.71  \n",
      "17   32.708389              5.678595       38.4         16.50  \n",
      "18   33.127011              4.795137       60.1          6.41  \n",
      "19   33.114298              4.096783       57.5          6.89  \n",
      "20   33.215823              2.716804       37.6         10.07  \n",
      "21   32.803904              3.881010        7.3          8.39  \n",
      "22   32.778288              4.385410       32.8          8.10  \n",
      "23   32.698600              5.368390       57.5         20.13  \n",
      "24   32.714833              5.608287       17.7          6.79  \n",
      "25   33.144675              3.588261       38.0          6.93  \n",
      "26   32.725282              5.555192       14.7         19.77  \n",
      "27   32.790827              3.845734       34.0         11.36  \n",
      "28   32.729343              5.224138        1.8          6.27  \n",
      "29   33.189692              4.217883       37.7          7.34  \n",
      "30   33.133533              3.976653       39.3          8.47  \n",
      "31   32.715185              6.509322        9.8          4.61  \n",
      "..         ...                   ...        ...           ...  \n",
      "590  32.771292              5.513371        1.5         10.14  \n",
      "591  33.034810              3.520135        2.5         11.36  \n",
      "592  32.825317              3.755546        1.6          2.50  \n",
      "593  32.989533              3.371882        5.5          7.09  \n",
      "594  32.951603              3.744757        2.7          3.72  \n",
      "595  32.926018              5.452033        2.6          3.92  \n",
      "596  32.905052              5.542221        2.4          7.92  \n",
      "597  33.008998              4.907751        2.9          4.41  \n",
      "598  33.019864              4.575363        0.6          9.77  \n",
      "599  33.081224              5.250564        3.1          7.02  \n",
      "600  33.001813              4.153757        2.5          6.56  \n",
      "601  33.077977              3.696511        2.5          7.88  \n",
      "602  33.015995              4.004897       10.0         15.78  \n",
      "603  32.858973              4.093631        0.9          4.74  \n",
      "604  32.951106              4.416079        3.7          3.58  \n",
      "605  33.163745              3.714452        3.5          6.10  \n",
      "606  32.775535              5.654351        3.3          7.49  \n",
      "607  32.769264              4.312435        1.1          4.14  \n",
      "608  32.753005              5.295173        1.1          8.34  \n",
      "609  32.825321              2.097243        1.8          9.49  \n",
      "610  33.351059              6.481199        0.8         19.80  \n",
      "611  32.845504              2.874148        1.3          6.07  \n",
      "612  32.967798              3.638810        3.6          3.48  \n",
      "613  32.787802              4.274129        5.5         10.48  \n",
      "614  33.003708              2.714985        3.5          3.04  \n",
      "615  32.769915              4.849444        4.0          5.39  \n",
      "617  32.817116              5.216501        2.1         12.40  \n",
      "618  32.798451              3.303271        3.9          3.10  \n",
      "620  33.001207              2.903605        5.2          4.95  \n",
      "621  32.657202              4.685795        1.6         13.31  \n",
      "\n",
      "[616 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# This df_census has only relevant info with na dropped.\n",
    "df_census = pd.read_csv('./Data/sandiegocensustract.csv')\n",
    "df_census = df_census[['TRACTNUM','SingMother','PovertyRt','TotalPopulation','Longitude','Latitude','PollutionBurdenScore','Education','Unemployment']]\n",
    "df_census = df_census.dropna()\n",
    "print (df_census)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get alcohol permit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            lat         lon\n",
      "0     32.777847 -117.248361\n",
      "1     32.916755 -117.123423\n",
      "2     32.701524 -117.113723\n",
      "3     32.749524 -117.117612\n",
      "4     32.800508 -117.236281\n",
      "5     32.788560 -117.237149\n",
      "6     32.711729 -117.104456\n",
      "7     32.651772 -117.097562\n",
      "8     25.945487 -136.178773\n",
      "9     32.701503 -117.115966\n",
      "10    33.170043 -117.096125\n",
      "11    33.129334 -117.089836\n",
      "12    33.043428 -117.294747\n",
      "13    32.748128 -117.148553\n",
      "14    32.705522 -117.134833\n",
      "15    33.099189 -117.002897\n",
      "16    32.798226 -117.220333\n",
      "17    25.945487 -136.178773\n",
      "18    32.913865 -117.130063\n",
      "19    32.593614 -117.046372\n",
      "20    33.202693 -117.388922\n",
      "21    33.198923 -117.364572\n",
      "22    32.742869 -117.041554\n",
      "23    32.977668 -117.230363\n",
      "24    33.034459 -117.063417\n",
      "25    33.032768 -117.273133\n",
      "26    32.719575 -117.173374\n",
      "27    25.945487 -136.178773\n",
      "28    32.625898 -117.031158\n",
      "29    33.046785 -116.633057\n",
      "...         ...         ...\n",
      "4957  33.121530 -117.082569\n",
      "4958  32.795212 -116.960862\n",
      "4959  33.094793 -117.056463\n",
      "4960  25.945487 -136.178773\n",
      "4961  32.741053 -117.252100\n",
      "4962  33.063621 -117.302312\n",
      "4963  32.960094 -117.111876\n",
      "4964  33.159105 -117.346298\n",
      "4965  33.093716 -117.058201\n",
      "4966  33.094793 -117.056463\n",
      "4967  32.894367 -117.201096\n",
      "4968  32.740499 -117.229039\n",
      "4969  33.135193 -117.190887\n",
      "4970  32.956946 -117.027028\n",
      "4971  32.651754 -117.097555\n",
      "4972  32.951369 -117.061301\n",
      "4973  32.765696 -117.196225\n",
      "4974  25.945487 -136.178773\n",
      "4975  33.177300 -117.339679\n",
      "4976  33.135853 -117.121085\n",
      "4977  32.977668 -117.230363\n",
      "4978  32.641850 -116.992346\n",
      "4979  32.726228 -117.169059\n",
      "4980  25.945487 -136.178773\n",
      "4981  32.978417 -117.250596\n",
      "4982  32.726228 -117.169059\n",
      "4983  32.668309 -117.107164\n",
      "4984  32.953331 -117.232644\n",
      "4985  32.789572 -117.022490\n",
      "4986  33.127880 -117.083809\n",
      "\n",
      "[4987 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# This df_alc only has lat and lon data with na dropped.\n",
    "df_alc = pd.read_csv('./Data/abs-licenses-casnd.csv')\n",
    "df_alc = df_alc[['lat','lon']]\n",
    "df_alc = df_alc.dropna()\n",
    "print (df_alc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we count the crimes that match street light locations. After downsizing our sample 100x, we found that crimes do not happen under street lights. The count we found was 0. Later we will increase the sample size for more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### This is O(n^2) loop that is too slow.\n",
    "# def check_lat_lon(a, b):\n",
    "#     return abs(a-b) <= 0.0001\n",
    "# count = 0\n",
    "# for i_1, r_1 in df_crime.iterrows():\n",
    "#     for i_2, r_2 in df_lights.iterrows():\n",
    "#         if(check_lat_lon(r_1['lat'],r_2['lat']) and check_lat_lon(r_1['lon'],r_2['lon'])):\n",
    "#             count += 1\n",
    "            \n",
    "# This is crime and lights data with lat and lon truncated to 4 digits after the decimal and also night only crime data. Also save untruncated crime night only.\n",
    "df_crime_trun4 = df_crime.round({'lat': 4, 'lon':4})\n",
    "df_crime_night = df_crime[df_crime['is_night'] == 1]\n",
    "df_crime_trun4_night_only = df_crime_trun4[df_crime_trun4['is_night'] == 1]\n",
    "df_lights_trun4 = df_lights.round({'lat': 4, 'lon':4})\n",
    "# print (df_crime_trun4_night_only)\n",
    "\n",
    "# This is crime data with only lat and long truncated to 4 digits and night only data.\n",
    "df_crime_trun4_night_only = df_crime_trun4_night_only[['lat','lon']]\n",
    "\n",
    "# Since our loop will take too long we have to sample only 8k data from crimes and 485 from street lights (downsized 100 times)\n",
    "df_crime_rand_trun4_night_only = df_crime_trun4_night_only.sample(8000)\n",
    "df_lights_rand_trun4 = df_lights_trun4.sample(485)\n",
    "# print (df_crime_rand_trun4_night_only)\n",
    "\n",
    "\n",
    "\n",
    "# new loop\n",
    "# counter = 0\n",
    "# count = 0\n",
    "# for i_1, r_1 in df_crime_rand_trun4_night_only.iterrows():\n",
    "#     counter += 485\n",
    "#     print (counter)\n",
    "#     for i_2, r_2 in df_lights_rand_trun4.iterrows(): \n",
    "#         if(r_1['lat'] == r_2['lat'] and r_1['lon'] == r_2['lon']):\n",
    "#             count += 1\n",
    "\n",
    "\n",
    "# print (count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the crimes that happen at night, street lamp locations, and alcohol permit locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gmap functions\n",
    "import gmaps\n",
    "import gmaps.datasets\n",
    "from itertools import product\n",
    "gmaps.configure(api_key = \"AIzaSyD7PjQ3edhgPcImWTk5lCcnJPyX7U_u0KU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730f6d3af6fe4665a6f2f62f39ec991c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### plot all night crimes\n",
    "numpy_crime_array = list(zip(df_crime_night['lat'].tolist(), df_crime_night['lon'].tolist()))\n",
    "# print (numpy_crime_array)\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(gmaps.heatmap_layer(numpy_crime_array))\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2224853df84d0aaea7080ac278d638"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### plot all street lamp locations\n",
    "numpy_light_array = list(zip(df_lights['lat'].tolist(), df_lights['lon'].tolist()))\n",
    "# print (numpy_crime_array)\n",
    "fig2 = gmaps.figure()\n",
    "fig2.add_layer(gmaps.heatmap_layer(numpy_light_array))\n",
    "fig2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3d681826bb4dfc84017f813a3e6ea8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### plot all alcohol permit locations  !!!!!temp set to 1000, as original is too laggy!!!!\n",
    "# print (df_alc)\n",
    "# numpy_alc_array = list(zip(df_alc['lat'].tolist(), df_alc['lon'].tolist()))\n",
    "# print (numpy_crime_array)\n",
    "\n",
    "alc_layer = gmaps.symbol_layer(df_alc.sample(1000),fill_color=\"green\",stroke_color=\"green\", scale=2)\n",
    "fig3 = gmaps.figure()\n",
    "fig3.add_layer(alc_layer)\n",
    "fig3.add_layer(gmaps.heatmap_layer(numpy_crime_array))\n",
    "fig3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Analysis and Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we will use the analyize the San Diego Census Tract info. We will lopopk at Single Mother Rate, Poverty Rate, Lat, Lon, Pollution Burden Score, Education, and Unemployment Rate. The hard part is we do not have variables given in the census tract and crime data files to compare. There are lat/lngs in both datasets however we cannot directly compare because in census tract dataset, lat/lng are not descriptive enough, the lat/lng are for the region that the data was taken in. We considered doing radius approximations similar to street lights but the regions are not equal in size nor shape. We looked through both datasets to see what other information can be used to correlate the data. Then we found there is a tract number inside the SD Census data. So, we have decided to use an API, the Census Block Conversions API https://www.fcc.gov/general/census-block-conversions-api to convert lat/lng to tract numbers and thus correlate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Credit to: https://github.com/waddell\n",
    "\n",
    "#Use an FCC API to convert lat, lng to census block\n",
    "url = 'http://data.fcc.gov/api/block/find?format=json&latitude='\n",
    "\n",
    "# define the new geolocation fields for our dataframe\n",
    "df_census['blockfips'] = 0\n",
    "# print (df_crime_rand_trun4)\n",
    "\n",
    "\n",
    "df_crime_rand_trun4 = df_crime_trun4.sample(1)\n",
    "\n",
    "#We need to iterate over the rows of the DataFrame and get data from the FCC API for each\n",
    "for i, row in df_crime_rand_trun4.iterrows():\n",
    "    resp = requests.get(url+str(row['lat'])+'&longitude='+str(row['lon']))\n",
    "    data = json.loads(resp.text)\n",
    "    \n",
    "#     print (df_census['TRACTNUM'])\n",
    "    print (data['Block']['FIPS'][1:11])\n",
    "    try:\n",
    "        df_census[str(df_census['TRACTNUM']) == data['Block']['FIPS'][1:10]]['blockfips'] += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print (df_census[df_census['blockfips'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This method is too slow, we have found another way to process the TRACTNUM with the lat and lon. We have used Texas A&M's geoservices to generate a file with lat/lng/TRACTNUMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_crime = pd.read_csv('./Data/final_crime.csv')\n",
    "# df_all_crime = df_all_crime[['latitude','longitude']]\n",
    "df_all_crime['tract'] = df_all_crime['CensusStateFips']*1000000000 + df_all_crime['CensusCountyFips']*1000000 + df_all_crime['CensusTract'] * 100\n",
    "df_all_crime = df_all_crime[['latitude','longitude','tract']]\n",
    "print (df_all_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census['count'] = 0\n",
    "print (df_census)\n",
    "# type(df_census['TRACTNUM'].index[0])\n",
    "type(df_all_crime['tract'].index[0])\n",
    "for iter, row in df_all_crime.iterrows():\n",
    "    for iter2, row2 in df_census.iterrows():\n",
    "        if row2['TRACTNUM'] == row['tract']:\n",
    "            row2['count'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Conclusions/Discussion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
